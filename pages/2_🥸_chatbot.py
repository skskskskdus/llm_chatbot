import streamlit as st
import zipfile
import json
import os
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import Chroma

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain.docstore.document import Document
from streamlit_extras.let_it_rain import rain
from langchain_core.callbacks.base import BaseCallbackHandler
from langchain.chains import ConversationChain
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.output_parsers import StrOutputParser

import time
from glob import glob
from dotenv import load_dotenv

# API í‚¤ ì •ë³´ ë¡œë“œ
load_dotenv()

# OpenAI API í‚¤ ì„¤ì •
OPENAI_API_KEY = "YOUR_API_KEY" # ì‹¤ì œ API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="chatbot", page_icon="ğŸ¥¸")
st.title('ğŸˆâ€â¬›ë‚˜ë§Œì˜ ì§‘ì‚¬ë‹˜ğŸˆâ€â¬›')

# ì¸¡ë©´ ë°”ì— ë¹„ë””ì˜¤ ì¶”ê°€
st.sidebar.video("https://youtu.be/FoO7Pmx0bE4")

# ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
if "model" not in st.session_state:
    st.session_state["model"] = "gpt-3.5-turbo"

# ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# ZIP íŒŒì¼ í•´ì œ ë° JSON ë°ì´í„° ì½ê¸°
zip_file_path = os.path.join("ai_data", "TL_02. ì¶”ì²œì§ì—… ì¹´í…Œê³ ë¦¬_01. ê¸°ìˆ ê³„ì—´.zip")
extract_dir = os.path.join("data", "data")
json_file_path = os.path.join(extract_dir, "ì „ë¬¸ê°€_ë¼ë²¨ë§_ë°ì´í„°_ê¸°ìˆ ê³„ì—´.json")

if "retriever" not in st.session_state:

    loader=DirectoryLoader("data",glob="*.json",loader_cls=TextLoader)
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    texts = text_splitter.split_documents(documents)
    from langchain_openai import OpenAIEmbeddings
    embeddings_model=OpenAIEmbeddings()
    embedding = embeddings_model
    vectordb = Chroma.from_documents(
        documents=texts,
        embedding=embedding)
    st.session_state.retriever = vectordb.as_retriever()

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
prompt = ChatPromptTemplate.from_template(
        """
    ë„ˆëŠ” ì§„ë¡œ ìƒë‹´ì„ ìœ„í•œ ì±—ë´‡ì´ì•¼. 
    ê¸°ìˆ  ê³„ì—´ ìƒë‹´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•  ìˆ˜ ìˆë„ë¡ í•™ìŠµë˜ì—ˆì–´. 
    ìƒë‹´ ë°ì´í„° ì™¸ì˜ ì§ˆë¬¸ì€ OpenAIì˜ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•  ìˆ˜ ìˆë„ë¡ ë˜ì–´ ìˆì–´.

    Answer the question based only on the following context:
    {context}

    Question: {question}
    """
)

def format_docs(docs):
    return '\n\n'.join(doc.page_content for doc in docs)

llm = ChatOpenAI(api_key=OPENAI_API_KEY,model_name="gpt-3.5-turbo", temperature=0)

# RAG Chain ì—°ê²°
rag_chain = (
    {'context':  st.session_state.retriever | format_docs, 'question': RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'conversation' not in st.session_state:
    st.session_state.conversation = []

# ì‘ë‹µ ìƒì„± í•¨ìˆ˜ ìˆ˜ì •
def generate_response(input_text):
    input_string = str(input_text)
    # ì´ì „ ëŒ€í™”ë¥¼ í¬í•¨í•˜ëŠ” BaseMessages ëª©ë¡ ìƒì„±
    #base_messages = create_base_messages(st.session_state.conversation)
    # RAG ì²´ì¸ì— ì „ë‹¬í•˜ì—¬ ì‘ë‹µ ìƒì„±
    response = rag_chain
    return response


# ì§ˆë¬¸ ì–‘ì‹
with st.form('Question'):
    text = st.text_area('ì˜¤ì¡°ì‚¬ë§ˆ ì…ë ¥:', placeholder='ì•¼ë ˆ ì•¼ë ˆ ë˜ ì§ˆë¬¸í•˜ëŠ” ê±°ì˜ˆìš”?')
    submitted = st.form_submit_button("ì§ˆë¬¸í•´ë³¼ê¹Œìš”?")
    if submitted:
        response = generate_response(text)
        st.session_state.conversation.insert(0, {'question': text, 'response': response})

# ëŒ€í™” ê¸°ë¡ í‘œì‹œ
for chat in st.session_state.conversation:
    st.write(f"**ì˜¤ì¡°ì‚¬ë§ˆ:** {chat['question']}")
    st.info(f"**ì§‘ì‚¬:** {chat['response']}")

# ëŒ€í™” ì €ì¥ ë²„íŠ¼
if st.button("ëŒ€í™” ì €ì¥"):
    if not os.path.exists('conversations'):
        os.makedirs('conversations')
    
    file_path = os.path.join('conversations', 'conversation.py')
    
    # íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš° ì´ì „ ëŒ€í™” ë¶ˆëŸ¬ì˜¤ê¸°
    if os.path.exists(file_path):
        with open(file_path, "r", encoding='utf-8') as file:
            exec(file.read(), globals(), locals())
    
    with open(file_path, "w", encoding='utf-8') as file:
        file.write("# ëŒ€í™” ê¸°ë¡\n")
        file.write("conversation = [\n")
        for chat in st.session_state.conversation:
            file.write(f"    {{'question': '''{chat['question']}''', 'response': '''{chat['response']}'''}},\n")
        if 'conversation' in locals():
            for chat in conversation:
                file.write(f"    {{'question': '''{chat['question']}''', 'response': '''{chat['response']}'''}},\n")
        file.write("]\n")
    st.success(f"ëŒ€í™”ê°€ 'conversations/conversation.py' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")

# ì €ì¥ëœ ëŒ€í™” í‘œì‹œ ë²„íŠ¼
if st.button("ì €ì¥ëœ ëŒ€í™” í‘œì‹œ"):
    try:
        with open(os.path.join('conversations', 'conversation.py'), "r", encoding='utf-8') as file:
            exec(file.read(), globals(), locals())
            if 'conversation' in locals():
                for chat in conversation:
                    st.write(f"**ì˜¤ì¡°ì‚¬ë§ˆ:** {chat['question']}")
                    st.info(f"**ì§‘ì‚¬:** {chat['response']}")
            else:
                st.error("íŒŒì¼ì—ì„œ ì €ì¥ëœ ëŒ€í™”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except FileNotFoundError:
        st.error("ì €ì¥ëœ ëŒ€í™” íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        st.error(f"íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

# ì‹œê°ì  íš¨ê³¼ í•¨ìˆ˜
def rose():
    rain(
        emoji="ğŸŒ¹",
        font_size=54,
        falling_speed=5,
        animation_length="infinite",
    )

rose()



