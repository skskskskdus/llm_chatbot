import sqlite3
#if sqlite3.sqlite_version_info < (3, 35, 0):
__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
from langchain_community.vectorstores import Chroma
import streamlit as st
import zipfile
import json
import os
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.prompts import ChatPromptTemplate
from langchain.chains import ConversationChain
from langchain.llms import OpenAI
from langchain.docstore.document import Document
from streamlit_extras.let_it_rain import rain
from langchain.output_parsers import PydanticOutputParser
from dotenv import load_dotenv
from glob import glob

import time

# OpenAI API í‚¤ ì„¤ì •
# API í‚¤ ì •ë³´ ë¡œë“œ
load_dotenv()

# OpenAI API í‚¤ ì„¤ì •
OPENAI_API_KEY =   "sk-proj-GwrYmjDxYJGOSRQifC8WT3BlbkFJ1fRQajPQ5Qu6o5jfO9S9"
 # ì‹¤ì œ API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

# íŽ˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="chatbot", page_icon="ðŸ¥¸")
st.title('ðŸˆâ€â¬›ë‚˜ë§Œì˜ ì§‘ì‚¬ë‹˜ðŸˆâ€â¬›')

# ì¸¡ë©´ ë°”ì— ë¹„ë””ì˜¤ ì¶”ê°€
st.sidebar.video("https://youtu.be/FoO7Pmx0bE4")

# ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
if "model" not in st.session_state:
    st.session_state["model"] = "gpt-4o"

# ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# ZIP íŒŒì¼ í•´ì œ ë° JSON ë°ì´í„° ì½ê¸°
#extract_dir = os.path.join("data", "data")
#json_file_path = os.path.join(extract_dir, "ì „ë¬¸ê°€_ë¼ë²¨ë§_ë°ì´í„°_ê¸°ìˆ ê³„ì—´_ing.json")
#json_file_path = os.path.join("ì „ë¬¸ê°€_ë¼ë²¨ë§_ë°ì´í„°_ê¸°ìˆ ê³„ì—´_ing.json")

# ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# ZIP íŒŒì¼ í•´ì œ ë° JSON ë°ì´í„° ì½ê¸°
    #extract_dir = os.path.join("data", "data")
    #json_file_path = os.path.join(extract_dir, "ì „ë¬¸ê°€_ë¼ë²¨ë§_ë°ì´í„°_ê¸°ìˆ ê³„ì—´_ing.json")

if "retriever" not in st.session_state:

    #json_file_path = os.path.join('llm_chatbot', "ì „ë¬¸ê°€_ë¼ë²¨ë§_ë°ì´í„°_ê¸°ìˆ ê³„ì—´_ing.json")

   # ë””ë ‰í† ë¦¬ ë‚´ì˜ ëª¨ë“  JSON íŒŒì¼ ê²½ë¡œë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ê°€ì ¸ì˜¤ê¸°,json_files = glob(os.path.join('', '*.json'))
    json_files = glob(os.path.join('llm_chatbot', 'ì „ë¬¸ê°€_ë¼ë²¨ë§_ë°ì´í„°_ê¸°ìˆ ê³„ì—´_ing.json'))

    # ëª¨ë“  JSON ë°ì´í„°ë¥¼ ì €ìž¥í•  ë¦¬ìŠ¤íŠ¸
    career_data = []

    # JSON íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ì™€ career_data ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
    for file in json_files:
        with open(file, 'r') as f:
            data = json.load(f)
            career_data.append(data)

    # JSON ë°ì´í„°ë¥¼ Document ê°ì²´ë¡œ ë³€í™˜
    documents = [Document(page_content=json.dumps(item, ensure_ascii=False)) for item in career_data]

    # í…ìŠ¤íŠ¸ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)
    splits = text_splitter.split_documents(documents)
    print("Chunks split Done.")
    
    # ìž„ë² ë”© ë° ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±, ê²€ìƒ‰
    embedding = OpenAIEmbeddings()
    vectordb = Chroma.from_documents(documents=splits, embedding=embedding)
    print("Retriever Done.")
    st.session_state.retriever = vectordb.as_retriever()
    #ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì„¸ì…˜ ìƒíƒœë¥¼ í™œìš©í•˜ì—¬ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì„¤ì •
# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
prompt = ChatPromptTemplate.from_template(
    """
    ë„ˆëŠ” ì§„ë¡œ ìƒë‹´ì„ ìœ„í•œ ì±—ë´‡ì´ì•¼. 
    ê¸°ìˆ  ê³„ì—´ ìƒë‹´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ì„œ ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ë‹µë³€í•  ìˆ˜ ìžˆë„ë¡ í•™ìŠµë˜ì—ˆì–´. 
    ìƒë‹´ ë°ì´í„° ì™¸ì˜ ì§ˆë¬¸ì€ OpenAIì˜ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•  ìˆ˜ ìžˆë„ë¡ ë˜ì–´ ìžˆì–´.

    Answer the question based only on the following context:
    {context}

    Question: {question}
    """
)
# ê²€ìƒ‰í•œ ë¬¸ì„œ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë¬¸ë‹¨ìœ¼ë¡œ í•©ì³ì¤ë‹ˆë‹¤.
def format_docs(docs):
    return '\n\n'.join(doc.page_content for doc in docs)
#llm ëª¨ë¸ ìƒì„±
llm = OpenAI(api_key=OPENAI_API_KEY, model="gpt-4o", temperature=0)

# RAG Chain ì—°ê²°
#ì„¸ì…˜ ìƒíƒœì—ì„œ ê²€ìƒ‰ê¸°(retriever)ë¥¼ ê°€ì ¸ì˜¤ê³ , format_docsëŠ” ë¬¸ì„œë¥¼ í˜•ì‹í™”í•©ë‹ˆë‹¤. RunnablePassthrough()ëŠ” ì§ˆë¬¸ì„ ì „ë‹¬
rag_chain = (
    {'context':  st.session_state.retriever | format_docs}
    | prompt
    | llm
)
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'conversation' not in st.session_state:
    st.session_state.conversation = []

# ì‘ë‹µ ìƒì„± í•¨ìˆ˜ ìˆ˜ì •
def generate_response(input_text):
    input_string = str(input_text)
    response = rag_chain.invoke(input_string)
    return response

# ì§ˆë¬¸ ì–‘ì‹
with st.form('Question'):
    text = st.text_area('ì˜¤ì¡°ì‚¬ë§ˆ ìž…ë ¥:', placeholder='ì•¼ë ˆ ì•¼ë ˆ ë˜ ì§ˆë¬¸í•˜ëŠ” ê±°ì˜ˆìš”?')
    submitted = st.form_submit_button("ì§ˆë¬¸í•´ë³¼ê¹Œìš”?")
    if submitted:
        response = generate_response(text)
        st.session_state.conversation.insert(0, {'question': text, 'response': response})

# ëŒ€í™” ê¸°ë¡ í‘œì‹œ
for chat in st.session_state.conversation:
    st.write(f"**ì˜¤ì¡°ì‚¬ë§ˆ:** {chat['question']}")
    st.info(f"**ì§‘ì‚¬:** {chat['response']}")

# ëŒ€í™” ì €ìž¥ ë²„íŠ¼
if st.button("ëŒ€í™” ì €ìž¥"):
    if not os.path.exists('conversations'):
        os.makedirs('conversations')
    
    file_path = os.path.join('conversations', 'conversation.py')
    
    # íŒŒì¼ì´ ì¡´ìž¬í•˜ëŠ” ê²½ìš° ì´ì „ ëŒ€í™” ë¶ˆëŸ¬ì˜¤ê¸°
    if os.path.exists(file_path):
        with open(file_path, "r", encoding='utf-8') as file:
            exec(file.read(), globals(), locals())
    
    with open(file_path, "w", encoding='utf-8') as file:
        file.write("# ëŒ€í™” ê¸°ë¡\n")
        file.write("conversation = [\n")
        for chat in st.session_state.conversation:
            file.write(f"    {{'question': '''{chat['question']}''', 'response': '''{chat['response']}'''}},\n")
        if 'conversation' in locals():
            for chat in conversation:
                file.write(f"    {{'question': '''{chat['question']}''', 'response': '''{chat['response']}'''}},\n")
        file.write("]\n")
    st.success(f"ëŒ€í™”ê°€ 'conversations/conversation.py' íŒŒì¼ì— ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤!")

# ì €ìž¥ëœ ëŒ€í™” í‘œì‹œ ë²„íŠ¼
if st.button("ì €ìž¥ëœ ëŒ€í™” í‘œì‹œ"):
    try:
        with open(os.path.join('conversations', 'conversation.py'), "r", encoding='utf-8') as file:
            exec(file.read(), globals(), locals())
            if 'conversation' in locals():
                for chat in conversation:
                    st.write(f"**ì˜¤ì¡°ì‚¬ë§ˆ:** {chat['question']}")
                    st.info(f"**ì§‘ì‚¬:** {chat['response']}")
            else:
                st.error("íŒŒì¼ì—ì„œ ì €ìž¥ëœ ëŒ€í™”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except FileNotFoundError:
        st.error("ì €ìž¥ëœ ëŒ€í™” íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        st.error(f"íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

# ì‹œê°ì  íš¨ê³¼ í•¨ìˆ˜
def rose():
    rain(
        emoji="ðŸŒ¹",
        font_size=54,
        falling_speed=5,
        animation_length="infinite",
    )

rose()
